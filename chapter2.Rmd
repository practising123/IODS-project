# Chapter 2 - Regression and model validation

- Did the  dataCamp exercises.

#Wrangling code:
```{r,dpi = 200, message=FALSE}
library(dplyr)
library(ggplot2)
library(GGally)
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-
data.txt", sep="\t", header=TRUE)
dim(lrn14)
str(lrn14)

```

- Reading the table from web and saving to lrn14. 183 rows/persons, 60 variables.

```{r,dpi = 200, message=FALSE}
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
deep_columns <- select(lrn14, one_of(deep_questions))
deep <- rowMeans((deep_columns))
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
surface_columns <- select(lrn14, one_of(surface_questions))
surf <- rowMeans(surface_columns)
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
strategic_columns <- select(lrn14, one_of(strategic_questions))
stra <- rowMeans(strategic_columns)
```

- Taking courses(in deep, strategic...) and counting their means to create new combination variables.

```{r,dpi = 200, message=FALSE}

keep_columns <- c("gender","Age","Attitude", "Points")
learning2014 <- select(lrn14, one_of(keep_columns))
```
- Decide columns to take and retrieve all of them to "learning2014"
- Choosing the columns, creating new data frame learning2014 for analyzing.

```{r,dpi = 200, message=FALSE}

learning2014$deep <- deep
learning2014$stra <- stra
learning2014$surf <- surf
learning2014 <-filter(learning2014, Points>0)
str(learning2014)
dim(learning2014)
```
- Insert compilation variables and filtering zero-pointers. Now we have Learning table with keep columns and combination columns.

```{r,dpi = 200, message=FALSE}


write.table(learning2014, file = "data/learning2014.txt", row.names = TRUE)

new_table <- read.table(file = "data/learning2014.txt", header=TRUE)
dim(new_table)
str(new_table)
```

- Saving and loading from /data

#Analysis
2.1

```{r,dpi = 200, message=FALSE}

new_table <- read.table(file = "data/learning2014.txt", header=TRUE)
summary(new_table)
```
- The data contains study information on 166 individuals, with 7 variables: Age(years), gender (F/M), Attitude, points, deep, stra and surf.
- Attitude is mean of answers on multiple questions measuring students attitude towards statistics. 
- Deep, stra and surf are means of multiple questions measuring these subjects. 
- Points is exam points.

2.2
```{r,dpi = 200, message=FALSE}

p <- ggpairs(new_table, mapping = aes(col=gender, alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
```
- Creates the plot of new_table and prints it.
```{r,dpi = 200, message=FALSE}
table(cut(new_table$Age, breaks = seq(15,55,by=10)))
table(new_table$gender, cut(new_table$Attitude, breaks = seq(1,56,by=5)))

```
- Notes on data:
- Only 56/166 are men.
- Biggest correlations are between points and attitude 0.437 and negative correlation between deep and surg -0.324. This is because of males -0.622 correlation, female got only -0.087.
- Most of the students are between 15 and 25 years, on the youngest category. Mean is bigger then median so the distribution of age is skewed to the left.
- Shows that male have better attitude in general on statistics then female.

```{r,dpi = 200, message=FALSE}
```
2.3
```{r,dpi = 200, message=FALSE}
my_model <- lm(Points ~ Attitude + stra + surf, data = new_table)
summary(my_model)

```
- Attitude having 0.34 regression to points is significant, attitude has ***
- Multiple R-squared:  0.21 and 0.34 regression.
- Dropping stra and surf because those are not relevant.
```{r,dpi = 200, message=FALSE}

my_model <- lm(Points ~ Attitude, data = new_table)
```
2.4
```{r,dpi = 200, message=FALSE}
summary(my_model)
```
- Multiple R-squared:  0.19 and 0.35 regression. predictors explain 19% of variance. 1 point in attitude raises exam points by 0.353 on average.
```{r,dpi = 200, message=FALSE}


my_model <- lm(Points ~ Attitude, data = new_table)
summary(my_model)

```
2.5
- Assumptions of linear regression models:

- The errors are normally distributed

- The errors are not correlated

- The errors have constant variance

- The size of a given error does not depend on the explanatory variables

```{r,dpi = 200, message=FALSE}
plot(my_model, which =c(1))
```
- Residual vs fitted. Tells if there are patterns, patterns mean problem with the assumption. If there would be bigger residual on bigger values, the assumption(my_model) would need to be corrected. Residuals seem to be scattered evenly(variance seems constant). 
```{r,dpi = 200, message=FALSE}

plot(my_model, which =c(2))
```
- The Q-Q-plot most of the points are on line with little mix in the ends. Shows the error assumption. Seems good enough,
```{r,dpi = 200, message=FALSE}

plot(my_model, which =c(5))
```
- Tells if there are influentical cases in the model. if there are cases on top right or bottom right, those would strongly influence the results.
